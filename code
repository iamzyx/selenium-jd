#coding=utf-8
# 某些情况下pyquery并不好用
# 这个就是使用pyquery失败了，所以使用beautifulsoup
from urllib import parse
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
from selenium.common.exceptions import SessionNotCreatedException as SNCE
from selenium.common.exceptions import NoSuchElementException as NSEE
from selenium.common.exceptions import NoSuchWindowException as NSWE
from bs4 import BeautifulSoup
import time
import pymongo

def get_page_source(page_no):
    #chrome_options=webdriver.ChromeOptions()  无界面浏览器
    #chrome_options.add_argument("--headless")
    #browser=webdriver.Chrome(chrome_options=chrome_options)
    browser=webdriver.Chrome()
    browser.get(page_url)
    item_wait=WebDriverWait(browser,10)

    # 下拉到网页底部
    try:
        for i in range(100):
            browser.execute_script(
                "window.scrollTo(0,{0})".format(i * 200)
            )
        time.sleep(1)
        if page_no > 1:
            input_page = item_wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR,'#J_bottomPage > span.p-skip > input'))
            )
            input_page.clear()
            input_page.send_keys(page_no)
            submit=browser.find_element_by_css_selector('#J_bottomPage > span.p-skip > a')
            submit.click()
            time.sleep(2)

        # 下拉到网页底部
        for i in range(100):
            browser.execute_script(
                "window.scrollTo(0,{0})".format(i*100)
            )
        time.sleep(3)   # 等待加载完毕
        tmp_page_source=browser.page_source
        browser.quit()
        return tmp_page_source
    except SNCE:
        print(str(page_no)+" 抓取失败"+"SNCE")
    except NSEE:
        print(str(page_no) + " NSEE")
    except NSWE:
        print(str(page_no) + " NSWE")

def catch_page(text=''):
    soup=BeautifulSoup(text,"lxml")
    tmp1 = soup.select_one(".gl-warp.clearfix").select(".gl-item")

    for i in tmp1:
        link=i.select_one(".p-img").select_one("a").attrs["href"]   # 获取链接
        item_name=i.select_one(".p-name.p-name-type-2").select_one("em").text
        price=i.select_one(".p-price").select_one("i").text
        product={
            "产品名": item_name,
            "产品链接":link,
            "产品价格":price
        }
        print(product)
        save_mongo(product)

def save_mongo(result):
    try:
        if db[MOGO_COLLECTION].insert(result):
            print("保存成功")
    except:
        print("保存失败")

def main(page_no=1):
    for i in range(1,page_no+1):
        tmp2=get_page_source(page_no=i) # 获取网页源码
        catch_page(text=tmp2)   # 抓取网页信息


if __name__=='__main__':
    keyword = parse.quote("ipad")
    url = 'https://search.jd.com/Search?keyword='
    page_url=url+keyword
    MONGO_URL=""
    MONGO_DB="JD"
    MOGO_COLLECTION='products'
    client=pymongo.MongoClient(MONGO_URL)
    db=client[MONGO_DB]
    main(5)     # 抓取5个页面





